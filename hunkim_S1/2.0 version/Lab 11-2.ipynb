{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-1be9c0ed42d8>:1: read_data_sets (from input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as: tensorflow_datasets.load('mnist')\n",
      "WARNING:tensorflow:From /workspace/tftf/hunkim_S1/2.0 version/input_data.py:284: _maybe_download (from input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /workspace/tftf/hunkim_S1/2.0 version/input_data.py:286: _extract_images (from input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /workspace/tftf/hunkim_S1/2.0 version/input_data.py:291: _extract_labels (from input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /workspace/tftf/hunkim_S1/2.0 version/input_data.py:104: _dense_to_one_hot (from input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /workspace/tftf/hunkim_S1/2.0 version/input_data.py:315: _DataSet.__init__ (from input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/_DataSet.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "x_data = np.reshape(mnist.train.images, [-1, 28, 28, 1])\n",
    "y_data = mnist.train.labels\n",
    "\n",
    "x_test = np.reshape(mnist.test.images, [-1, 28, 28, 1])\n",
    "y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples\n",
      "Epoch 1/20\n",
      "55000/55000 [==============================] - 166s 3ms/sample - loss: 0.3633 - acc: 0.8806\n",
      "Epoch 2/20\n",
      "55000/55000 [==============================] - 164s 3ms/sample - loss: 0.1133 - acc: 0.9647\n",
      "Epoch 3/20\n",
      "55000/55000 [==============================] - 166s 3ms/sample - loss: 0.0886 - acc: 0.9724\n",
      "Epoch 4/20\n",
      "55000/55000 [==============================] - 163s 3ms/sample - loss: 0.0735 - acc: 0.9771\n",
      "Epoch 5/20\n",
      "55000/55000 [==============================] - 164s 3ms/sample - loss: 0.0646 - acc: 0.9797\n",
      "Epoch 6/20\n",
      "55000/55000 [==============================] - 163s 3ms/sample - loss: 0.0623 - acc: 0.9807\n",
      "Epoch 7/20\n",
      "55000/55000 [==============================] - 163s 3ms/sample - loss: 0.0558 - acc: 0.9821\n",
      "Epoch 8/20\n",
      "55000/55000 [==============================] - 163s 3ms/sample - loss: 0.0532 - acc: 0.9830\n",
      "Epoch 9/20\n",
      "55000/55000 [==============================] - 163s 3ms/sample - loss: 0.0491 - acc: 0.9846\n",
      "Epoch 10/20\n",
      "55000/55000 [==============================] - 165s 3ms/sample - loss: 0.0485 - acc: 0.9851\n",
      "Epoch 11/20\n",
      "55000/55000 [==============================] - 167s 3ms/sample - loss: 0.0465 - acc: 0.9852\n",
      "Epoch 12/20\n",
      "55000/55000 [==============================] - 165s 3ms/sample - loss: 0.0457 - acc: 0.9852\n",
      "Epoch 13/20\n",
      "55000/55000 [==============================] - 165s 3ms/sample - loss: 0.0412 - acc: 0.9865\n",
      "Epoch 14/20\n",
      "55000/55000 [==============================] - 164s 3ms/sample - loss: 0.0427 - acc: 0.9867\n",
      "Epoch 15/20\n",
      "55000/55000 [==============================] - 166s 3ms/sample - loss: 0.0425 - acc: 0.9867\n",
      "Epoch 16/20\n",
      "55000/55000 [==============================] - 163s 3ms/sample - loss: 0.0375 - acc: 0.9881\n",
      "Epoch 17/20\n",
      "55000/55000 [==============================] - 163s 3ms/sample - loss: 0.0386 - acc: 0.9878\n",
      "Epoch 18/20\n",
      "55000/55000 [==============================] - 162s 3ms/sample - loss: 0.0373 - acc: 0.9878\n",
      "Epoch 19/20\n",
      "55000/55000 [==============================] - 163s 3ms/sample - loss: 0.0368 - acc: 0.9880\n",
      "Epoch 20/20\n",
      "55000/55000 [==============================] - 163s 3ms/sample - loss: 0.0397 - acc: 0.9877\n",
      "Train ACC :  0.9978182\n",
      "Test ACC :  0.9937\n"
     ]
    }
   ],
   "source": [
    "rate = 0.3\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=32, \n",
    "                           kernel_size=(3, 3), \n",
    "                           strides=(1, 1),\n",
    "                           activation='relu', \n",
    "                           kernel_initializer=tf.initializers.RandomNormal(stddev=0.01)),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), \n",
    "                              strides=(2, 2), \n",
    "                              padding='same'),\n",
    "    tf.keras.layers.Dropout(rate),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(filters=64, \n",
    "                           kernel_size=(3, 3), \n",
    "                           strides=(1, 1),\n",
    "                           activation='relu', \n",
    "                           kernel_initializer=tf.initializers.RandomNormal(stddev=0.01)),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), \n",
    "                              strides=(2, 2), \n",
    "                              padding='same'),\n",
    "    tf.keras.layers.Dropout(rate),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(filters=128, \n",
    "                           kernel_size=(3, 3), \n",
    "                           strides=(1, 1),\n",
    "                           activation='relu', \n",
    "                           kernel_initializer=tf.initializers.RandomNormal(stddev=0.01)),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), \n",
    "                              strides=(2, 2), \n",
    "                              padding='same'),\n",
    "    tf.keras.layers.Dropout(rate),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(625, kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                          activation='linear', dtype='float32'),\n",
    "    tf.keras.layers.Dropout(rate),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                          activation='softmax', dtype='float32')\n",
    "])\n",
    "\n",
    "adam = tf.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=adam, \n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_data, y_data, batch_size=100, verbose=1, epochs=20)\n",
    "\n",
    "hypothesis = model.predict(x_data)\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(y_data, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))  \n",
    "\n",
    "print(\"Train ACC : \", accuracy.numpy())\n",
    "\n",
    "hypothesis = model.predict(x_test)\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(y_test, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))  \n",
    "\n",
    "print(\"Test ACC : \", accuracy.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
